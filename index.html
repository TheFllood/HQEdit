<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing</title>


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./resources/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span>Mude Hui</a></span></sup>,</span>
            <span class="author-block">
              <span>Siwei Yang</a></span></sup>,</span>
            <span class="author-block">
              <span>Bingchen Zhao</a></span></sup>,</span>
            </span>
            <span class="author-block">
              <span>Yichun Shi</a></span></sup>,</span>
            </span>
            <span class="author-block">
              <span>Heng Wang</a></span></sup>,</span>
            </span>
            <span class="author-block">
              <span>Peng Wang</a></span></sup>,</span>
            </span>
            <span class="author-block">
              <span>Yuyin Zhou</a></span></sup>,</span>
            </span>
            <span class="author-block">
              <span>Cihang Xie</a></span></sup>,</span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Santa Cruz</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.09990"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solidassasas fa-face-smiling-hands"></i>
                    <img src="./resources/ar.svg" alt="img" style="width: 100%; height: 100%" /> 
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UCSC-VLAA/HQ-Edit"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solidasasa fa-face-smiling-hands"></i>
                   <img src="./resources/hg.svg" alt="img" style="width: 100%; height: 100%" /> 
                  </span>
                   <span> Data</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/LAOS-Y/HQEdit"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-solidassasas fa-face-smiling-hands"></i>
                      <img src="./resources/gr.svg" alt="img" style="width: 100%; height: 100%" /> 
                    </span>
                    <span>Demo</span>
                    </a>
                  </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <img src="./resources/teaser.png" alt="alt text"
                        style="width: 100%; object-fit: cover; max-width:100%;"></a>
      <h2 class="subtitle has-text-centered">
        (a) - (d): example images and edit instructions from HQ-Edit. (e) we compare the dataset quality
        between our HQ-Edit and existing ones. Note that ``Alignment'' and ``Coherence'' are our newly
        developed metrics for measuring image/text qualities.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container ">
    <div class="hero-body">
      <center><h2 class="title is-3">Demo</h2></center>
  <iframe src="https://laos-y-hqedit.hf.space" frameborder="0" width="100%" height="1000"></iframe>
</div>
</div>
</section>

<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study introduces HQ-Edit, a high-quality instruction-based image editing dataset
            with around 200,000 edits. Unlike prior approaches relying on attribute guidance or
            human feedback on building datasets, we devise a scalable data collection pipeline
            leveraging advanced foundation models, namely GPT-4V and DALL-E 3. To ensure its high
            quality,
            diverse examples are first collected online, expanded, and then used to create
            high-quality diptychs featuring input and output images with detailed text prompts,
            followed by precise alignment ensured through post-processing.
            In addition, we propose two evaluation metrics, Alignment and Coherence, to
            quantitatively assess the quality of image edit pairs using GPT-4V.
            HQ-Editâ€™s high-resolution images, rich in detail and accompanied by comprehensive
            editing prompts, substantially enhance the capabilities of existing image editing
            models.
            For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image
            editing performance, even surpassing those models fine-tuned with human-annotated data.
            Dataset and models are available at <a
                href=https://github.com/UCSC-VLAA/HQ-Edit>[HQ-Edit]</a>
          </p>
        </div>
      </div>
    </div>
  </section>
    <!--/ Abstract. -->

    <section class="section">
      <div class="container">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">HQ-Edit Method</h2>
            <div class="content has-text-justified">
              <center><img class="center" src="./resources/method.png" width="100%"></center>
              <p>
                <strong style="font-weight: 900">Method Overview.</strong>
                Our method consists of three steps: (1)Expansion: Massively generating image descriptions and
                edit instructions based on seed samples using GPT-4. (2)Generation: Generating diptychs using
                GPT-4V and DALL-E according to image descriptions and instructions. (3)Post-Processing:
                Post-process diptychs and edit instructions with GPT-4V and other various methods to produce
                image pairs and further enhance the quality of the dataset in different aspects.
              </p>
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <div class="container">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">HQ-Edit Dataset</h2>
              <div class="content has-text-justified">
                <center><img class="center" src="./resources/data_sample.png" width="100%"></center>
                <p>
                  <strong style="font-weight: 900">HQ-Edit Data Sample.</strong style="font-weight: 900">
                    Example data sampled from HQ-Edit. Our data contains two main parts, Instruction (input, edit,
                    inverse-edit, output) and Image (input image, output image). The two samples highlight that, 1)
                    the image is densely packed with details, 2) the input and ouput offers a comprehensive
                    description of the input and output image, and 3) the edit and inverse-edit instructions
                    precisely delineate the transformations occurring between the two images.
                </p>
              </div>
            </div>
          </div>
        </section>

        <section class="section">
          <div class="container">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Dataset Statistics Results</h2>
                <div class="content has-text-justified">
                  <center><img class="center" src="./resources/round.png" width="100%"></center>
                  <p>
                    <strong style="font-weight: 900">HQ-Edit Data Sample.</strong style="font-weight: 900">
                      Distribution of edit types and keywords in instructions. The inner ring depicts the types of
                      edit instructions and the outer circle shows the frequencies of instruction keywords. This
                      demonstrates the rich diversity contained within our instructions.
                  </p>
                </div>
              </div>
            </div>
          </section>

          <section class="section">
            <div class="container">
              <!-- Abstract. -->
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <h2 class="title is-3">Results of InstructPix2Pix Finetuned on HQ-Edit</h2>
                  <div class="content has-text-justified">
                    <center><img class="center" src="./resources/quantitative.png" width="100%"></center>
                    <p>
                      <strong style="font-weight: 900">HQ-Edit Data Sample.</strong style="font-weight: 900">
                        Qualitative comparison of InstructPix2Pix, MagicBrush, HIVE and HQ-Edit.
                        HQ-Edit demonstrates a more comprehensive diversity of editing instructions and possesses the
                        capability to manipulate images with greater precision and detail.
                    </p>
                  </div>
                </div>
              </div>
            </section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>

    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Based on the following <a href="http://nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
